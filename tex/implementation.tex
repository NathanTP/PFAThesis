The PFA was implemented within the RISC-V ecosystem. RISC-V is an open-source
instruction set with several open and closed-source implementations and ports
for many common software components\cite{riscv}. I used the RISC-V port of
Linux 4.13 with a Buildroot generated user space.

\subsection{PFA Reference Implementation}
To accelerate software development, and to provide a golden-model of
PFA behavior, I implemented the PFA first in a RISC-V ISA simulator called
"Spike"\cite{spike}. Spike provides a functional simulation of a RISC-V core
through a straightforward C++ interpreter, but does not provide any timing
accuracy. Due to its simplicity, the PFA implementation required only a few
weeks of implementation effort and less than 1000 LoC. With Spike, software
development was able to proceed concurrently with the concrete hardware design.
Furthermore, unit tests developed under Spike were used to validate the
hardware implementation, reducing debugging effort. In all, the only software
change that was needed to go from Spike to a concrete implementation was one
extra TLB flush due to a difference in TLB design between Spike and the RISC-V
implementation we used.

\subsection{Hardware Implementation}
The PFA prototype was implemented in the Chisel hardware construction
language\cite{chisel} and integrated with a simple in-order CPU called
RocketCore\cite{rocketchip}. The components were integrated using the
RocketChip system-on-chip (SoC) generator\cite{rocketchip}. I provide an
overview of the relevant systems in the following sections. The current PFA
prototype implements a subset of the specification described in Section
\ref{sec:pfaDesign}. Specifically, it does not support multiple simultaneous
evictions (the EvictQ has an effective depth of 1), however, it does allow for
asynchronous eviction. This prevents optimizations such as switching to other
threads while many pages are simultaneously evicted (a single eviction does not
take long enough to justify a context switch). The current prototype also does
not support the full memory blade specification. In particular, it does not
implement Ethernet headers which limits experiments to a single client/memory
pair over a static network route. Instead, it sends a simplified form of packet
header with only opcode and transaction ID. It also does not use a write
acknowledgment, this means the OS must rate-limit writes to avoid overwhelming
the memory blade. Without back-pressure, this rate-limiting must be
pessimistic.  However, the rate-limit only effects evictions which tend to
occur slowly over time.
 
\subsubsection{RocketCore and RocketChip}
RocketChip\cite{rocketchip} is a framework for generating SoCs. It includes
on-chip interconnects, caches, and other utilities for chip construction. While
the CPU is pluggable, we use only a single RocketCore in-order CPU for our
experiments. Our implementation used dedicated 16KB instruction and data
caches. While the node had access to several gigabytes of main memory, we
artificially limited application memory using Linux cgroups (see Section
\ref{sec:eval} for details). A real \gls{wsc} would likely include a mixture of
simple cores, high single-thread performance cores (e.g., out of order), and
accelerators. We hope to evaluate the Berkeley out-of-order core
(BOOM\cite{BOOM}) and the Hwatcha vector accelerator\cite{hwatcha} in the
future when they become available in our simulation infrastructure.

\subsubsection{Memory Blade}
The memory blade that is used by the PFA was implemented as a bare-metal C
program running on a RocketCore-based compute node. This memory blade is
connected to the same network as our PFA-enabled compute node and traverses a
simple switch in order to communicate. The current implementation uses a simple
protocol in lieu of a full Ethernet stack. We are currently evaluating
competing memory blade designs ranging from full hardware, to higher-level
software implementations. However, this simple design is sufficient for
evaluation of the core PFA functionality.

\subsubsection{FireSim}
We simulated the RTL using a cycle-accurate simulator called
"FireSim"\cite{firesim}. FireSim is an FPGA-accelerated simulator that runs on
the Amazon cloud. It can simulate thousands of nodes with a cycle-accurate
network and heterogeneous components.  Many parameters of the simulation are
tunable within FireSim. We decided on a \SI{200}{\giga\bit\per\second} network
with \SI{2}{\micro\second} link latency, leading to roughly 8us page access
time to the memory blade. The network is in-order and reliable. A realistic
Firebox system would include a much faster network with many more components
interacting. We limit ourselves to this simple design in order to evaluate only
fully synthesizable components (especially the limited throughput of our CPU
platform). Future experiments will evaluate the effects of faster networks with 
improved compute and memory performance

\begin{figure}[h]
\centering
\begin{tabular}{| l | l |}
  \hline
  \textbf{CPU Type} & Rocket (5-stage in order) \\ \hline
  \textbf{CPU Frequency} & \SI{3.2}{\giga\hertz} \\ \hline
  \textbf{Caches} & \SI{16}{\kilo\byte} D\$ and I\$ \\ \hline
  \textbf{NW Topology} & Single Switch \\ \hline
  \textbf{NW Bandwidth} & \SI{200}{\giga\bit\per\second} \\ \hline
  \textbf{NW Link Latency} & \SI{2}{\micro\second} \\ \hline
\end{tabular}
\caption{System parameters used for evaluation.}
\label{tbl:sim_parameters}
\end{figure}

\subsection{Linux Integration} \label{sec:linuxImpl}
    \input{tex/linuxDesign.tex}
